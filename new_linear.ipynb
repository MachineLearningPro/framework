{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_implement_linear import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_path=\"train.csv\"\n",
    "test_path=\"test.csv\"\n",
    "name=\"discrimination_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yb_train,data_train,ids_train=load_csv_data(train_path, sub_sample=False)\n",
    "#yb_test,data_test,ids_test=load_csv_data(test_path, sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  主函数的主体在这里\n",
    "    \n",
    "def main(data_train,yb_train, opt_model, cros_vali, degree,lambda_,gamma):\n",
    "    seed = np.random.randint(cros_vali*100)\n",
    "    data_train_copy = data_train.copy()\n",
    "    print('Started the run!\\n')\n",
    "    \n",
    "#     yb_train,data_train,ids_train=load_csv_data(train_path, sub_sample=False)\n",
    "    #yb_test,data_test,ids_test=load_csv_data(test_path, sub_sample=False)\n",
    "    \n",
    "    data_train_copy = data_train.copy()\n",
    "    print('Finished loading data!\\n')   \n",
    "    \n",
    "\n",
    "    print('Feature restore...')\n",
    "   \n",
    "    # data_train,u_s,u_b=restore_train_first_or_none_and_second(yb_train,data_train,train_size,1)\n",
    "    #data_train=restore_train_initial(yb_train,data_train,train_size)\n",
    "   \n",
    "    # data_train=restore_test_first_or_none(yb_train,data_train,train_size)\n",
    "    # data_train=restore_test_all_b_s(yb_train,data_train,train_size,u_s)\n",
    "    \n",
    "    print('Feature processing...\\n')\n",
    "    # mean_x = np.mean(data_train,0)\n",
    "    # data_train = data_train - mean_x\n",
    "    #std_x = np.std(data_train,0)\n",
    "    # data_train = data_train / std_x\n",
    "    \n",
    "    data_train = add_feature(data_train)\n",
    "    \n",
    "    \n",
    "    #data_train=drop_feature(data_train)\n",
    "    #degree=3\n",
    "    #a22_value=0;\n",
    "    #data_train=drop_feature(data_train)\n",
    "    #yb_train,data_train=drop_22(a22_value,yb_train,data_train)\n",
    "    #drop_22(2,yb_test,data_test)\n",
    "    #add_feature(data_train)\n",
    "    #add_feature(data_test)\n",
    "    \n",
    "    data_train = build_model_data(data_train, degree)\n",
    "    \n",
    "    #drop_feature(data_test)    \n",
    "    print('Training begin...\\n')\n",
    "    \n",
    "    print('Cross validation begins...\\n')\n",
    "    indices = build_k_indices(yb_train, cros_vali, seed) \n",
    "    corrections = cross_validation(data_train, yb_train, indices, opt_model)\n",
    "    \n",
    "    print(\"corrections\")\n",
    "    print(corrections)\n",
    "    \n",
    "    #data_train=destroy(yb_train,data_train)\n",
    "    \n",
    "    #yb_initial=yb_test.copy()\n",
    "#     sign1=yb_pred+yb_id\n",
    "#     sign2=yb_pred-yb_id\n",
    "#     ratio=len(sign1[sign1==2])/len(sign2[sign2==2])\n",
    "    \n",
    "    return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unified model choose\n",
    "def train_mode(train_x, train_y, chosen_model):\n",
    "    if chosen_model   == 1:     \n",
    "        w,loss=train_normal(train_y,train_x)\n",
    "    \n",
    "    elif chosen_model == 2:\n",
    "        w,loss=train_ridge(train_y, train_x, lambda_)\n",
    "    \n",
    "    elif chosen_model == 3:\n",
    "        w,loss=train_gradient(train_y, train_x)\n",
    "    \n",
    "    elif chosen_model == 4:\n",
    "        w,loss=train_logistic(train_y, train_x, gamma)\n",
    "        \n",
    "    elif chosen_model == 5:\n",
    "        w,loss=train_gradient_SGD(train_y, train_x)\n",
    "                                   \n",
    "#     elif chosen_model == 6:\n",
    "        \n",
    "    elif chosen_model == 7:\n",
    "        class_1 = train_x[train_y == -1][:,1:train_x.shape[1]]\n",
    "        class_2 = train_x[train_y == +1][:,1:train_x.shape[1]]\n",
    "        \n",
    "        w, loss = Fisher_classifier(class_1, class_2)\n",
    "        \n",
    "    return w, loss\n",
    "\n",
    "\n",
    "# optimization models\n",
    "def train_normal(yb_train, data_train):\n",
    "    w,loss=least_squares(yb_train,data_train)\n",
    "    return w,loss\n",
    "\n",
    "def train_ridge(yb_train, data_train, lambda_):\n",
    "    w,loss=ridge_regression(yb_train, data_train, lambda_)\n",
    "    return w,loss\n",
    "\n",
    "def train_gradient(yb_train, data_train, step_size):\n",
    "    max_iters = 100\n",
    "    gamma = step_size\n",
    "    initial_w=np.zeros(len(data_train.T));\n",
    "    w,loss=least_squares_GD(yb_train, data_train, initial_w, max_iters, gamma)\n",
    "    return w,loss\n",
    "\n",
    "def train_logistic(yb_train, data_train, step_size):\n",
    "    initial_w=np.ones(len(data_train.T));\n",
    "    max_iters=300;\n",
    "    gamma=step_size;\n",
    "    w,loss=logistic_regression(yb_train, data_train, initial_w, max_iters, gamma)\n",
    "    return w,loss\n",
    "\n",
    "def train_gradient_SGD(yb_train, data_train, step_size):\n",
    "    batch_size=1;\n",
    "    max_iters = 50\n",
    "    gamma = step_size\n",
    "    initial_w=np.zeros(len(data_train.T));\n",
    "    w,loss=least_squares_SGD(yb_train, data_train, initial_w, max_iters, gamma,batch_size)\n",
    "    return w,loss\n",
    "\n",
    "# reserved def\n",
    " \n",
    "\n",
    "def Fisher_classifier(class_1, class_2):\n",
    "    # means\n",
    "    m_1 = np.mean(class_1, axis = 0)\n",
    "    m_2 = np.mean(class_2, axis = 0)\n",
    "    \n",
    "    # inner class1 dispersion\n",
    "    S_1 = (class_1 - m_1).T.dot(class_1 - m_1)\n",
    "    # inner class2 dispersion\n",
    "    S_2 = (class_2 - m_2).T.dot(class_2 - m_2)\n",
    "    # seperating level\n",
    "    S_W = S_1 + S_2\n",
    "    \n",
    "    # perpendicular vector of the separating hyperplane\n",
    "    w_star = np.linalg.inv(S_W.astype(float)).dot(m_2 - m_1)\n",
    "    \n",
    "    mapped_b = class_1.dot(w_star)\n",
    "    m_t_b = mapped_b.mean()\n",
    "    mapped_s = class_2.dot(w_star)\n",
    "    m_t_s = mapped_s.mean()\n",
    "\n",
    "    # intersection point of the separating hyperplane and the perpendicular vector \n",
    "    w0 = -0.5*(m_t_b + m_t_s)\n",
    "    \n",
    "    return np.insert(w_star, 0, w0), np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  具体的子函数，很多现在没有用了\n",
    "\n",
    "def test(yb_train,data_train,w,train_size,data_train_copy):\n",
    "    yb_train=yb_train[train_size:]\n",
    "    data_train=data_train[train_size:]\n",
    "    data_train_copy=data_train_copy[train_size:]\n",
    "    #y_pred=predict_labels(w, data_train)\n",
    "    \n",
    "    score = np.dot(data_train, w)\n",
    "    y_pred=score.copy()\n",
    "    y_pred[np.where(y_pred <= 0)] = -1\n",
    "    y_pred[np.where(y_pred > 0)] = 1\n",
    "    y_pred[np.where(data_train_copy[:,0]==-999)]=-1\n",
    "    recall,precision,accuracy=performance(y_pred,yb_train)\n",
    "    print(\"recall\")\n",
    "    print(recall)\n",
    "    print(\"precision\")\n",
    "    print(precision)\n",
    "    print(\"accuracy\")\n",
    "    print(accuracy)\n",
    "    print(\"s%\")\n",
    "    print(33*recall*precision)\n",
    "    return y_pred,yb_train\n",
    "\n",
    "def test_sub(yb_test,data_test,w,ids_test):\n",
    "    y_pred=predict_labels(w, data_test)\n",
    "    create_csv_submission(ids_test, y_pred, name)\n",
    "    recall,precision,accuracy=performance(y_pred,yb_test)\n",
    "    print(\"recall\")\n",
    "    print(recall)\n",
    "    \n",
    "    \n",
    "def restore_train_first_or_none_and_second(yb,data,train_size,maxiter):\n",
    "    yb_train=yb[:train_size]\n",
    "    data_train=data[:train_size]\n",
    "   # yb_train=yb;\n",
    "   # data_train=data;\n",
    "    \n",
    "    data_s=data_train[yb_train==1]\n",
    "    data_b=data_train[yb_train==-1]\n",
    "    \n",
    "    u_s=np.zeros(len(data_s))\n",
    "    u_b=np.zeros(len(data_b))\n",
    "    for i in range(len(data_train.T)):\n",
    "        slice_s=data_s[:,i]\n",
    "        u_s[i]=np.mean(slice_s[slice_s!=-999])\n",
    "        slice_s[np.where(slice_s==-999)]=u_s[i]\n",
    "        \n",
    "        slice_b=data_b[:,i]\n",
    "        u_b[i]=np.mean(slice_b[slice_b!=-999])\n",
    "        slice_b[np.where(slice_b==-999)]=u_b[i]\n",
    "        \n",
    "    data_train[yb_train==1]=data_s\n",
    "    data_train[yb_train==-1]=data_b\n",
    "    #print(\"iter_=\");\n",
    "    #print(iter_);\n",
    "    #print(np.abs(u_s_new-u_b_new)/u_s_new);\n",
    "    return data,u_s,u_b\n",
    "\n",
    "def restore_train_initial(yb,data,train_size):\n",
    "\n",
    "    yb_train=yb[train_size:]\n",
    "    data_train=data[train_size:]\n",
    "    \n",
    "    maxiter=1;\n",
    "    iter_=0;\n",
    "    u_old=np.ones(len(data_train.T))*-999\n",
    "    err=10;\n",
    "    #data_short=\n",
    "    u=np.mean(data_train,0)\n",
    "    #data_s=data_train[yb_train==1]\n",
    "    #data_b=data_train[yb_train==-1]\n",
    "    #u_s=np.mean(data_s,0)\n",
    "    #u_b=np.mean(data_b,0)\n",
    "    \n",
    "    while iter_<maxiter and err>1e-2:\n",
    "        u_new=np.mean(data_train,0)\n",
    "        for i in range(len(data_train.T)):\n",
    "            slice_=data_train[:,i]\n",
    "            slice_[np.where(slice_==u_old[i])]=u_new[i]\n",
    "        err=np.linalg.norm(np.linalg.norm(np.abs(u_new-u_old)/u_new))\n",
    "        u_old=u_new\n",
    "        iter_+=1\n",
    "    #print(\"iter_=\");\n",
    "    #print(iter_);\n",
    "   # print(np.abs(u_new-u)/u_new);\n",
    "    return data\n",
    "\n",
    "def restore_test_first_or_none(yb,data,train_size):\n",
    "\n",
    "    yb_test=yb[train_size:]\n",
    "    data_test=data[train_size:]\n",
    "    #data_s=data_train[yb_train==1]\n",
    "    #data_b=data_train[yb_train==-1]\n",
    "    #u_s=np.mean(data_s,0)\n",
    "    #u_b=np.mean(data_b,0)\n",
    "    for i in range(len(data_test.T)):\n",
    "        #slice_=data_s[:,i]\n",
    "        #slice_[np.where(slice_==-999)]=u_s[i]\n",
    "        #slice_=data_b[:,i]\n",
    "        #slice_[np.where(slice_==-999)]=u_b[i]\n",
    "        slice_=data_test[:,i]\n",
    "        \n",
    "        u=np.mean(slice_[slice_!=-999])\n",
    "        slice_[np.where(slice_==-999)]=u\n",
    "    #data_train[yb==1]=data_s\n",
    "    #data_train[yb==-1]=data_b\n",
    "    return data\n",
    "\n",
    "def restore_test_second(yb,data,train_size,u_s,u_b):\n",
    "    \n",
    "    yb_test=yb;\n",
    "    #yb_test=yb[train_size:]\n",
    "    data_test=data[train_size:]\n",
    "    \n",
    "    data_s=data_test[yb_test==1]\n",
    "    data_b=data_test[yb_test==-1]\n",
    "    for i in range(len(data_test.T)):\n",
    "        #slice_=data_s[:,i]\n",
    "        #slice_[np.where(slice_==-999)]=u_s[i]\n",
    "        #slice_=data_b[:,i]\n",
    "        #slice_[np.where(slice_==-999)]=u_b[i]\n",
    "        slice_s=data_s[:,i]\n",
    "        slice_s[np.where(slice_s==-999)]=u_s[i]\n",
    "        \n",
    "        slice_b=data_b[:,i]\n",
    "        slice_b[np.where(slice_b==-999)]=u_b[i]\n",
    "    data_test[yb_test==1]=data_s\n",
    "    data_test[yb_test==-1]=data_b\n",
    "    return data\n",
    "\n",
    "def restore_test_all_b_s(yb,data,train_size,value):\n",
    "\n",
    "    yb_test=yb[train_size:]\n",
    "    data_test=data[train_size:]\n",
    "    #data_s=data_train[yb_train==1]\n",
    "    #data_b=data_train[yb_train==-1]\n",
    "    #u_s=np.mean(data_s,0)\n",
    "    #u_b=np.mean(data_b,0)\n",
    "    for i in range(len(data_test.T)):\n",
    "        slice_=data_test[:,i]\n",
    "        slice_[np.where(slice_==-999)]=value[i]\n",
    "    #data_train[yb==1]=data_s\n",
    "    #data_train[yb==-1]=data_b\n",
    "    return data\n",
    "\n",
    "def drop_feature(data):\n",
    "\n",
    "    #data=np.delete(data,range(23,29),1)\n",
    "    data=np.delete(data,20,1)\n",
    "    data=np.delete(data,range(17,18),1)    \n",
    "    data=np.delete(data,range(15,17),1)\n",
    "    #data=np.delete(data,12,1)\n",
    "    data=np.delete(data,7,1)   \n",
    "    data=np.delete(data,3,1) \n",
    "    #data=np.delete(data,range(4,7),1)\n",
    "    \n",
    "#r_data=np.delete(r_data,17,1)\n",
    "#r_data=np.delete(r_data,18,1)\n",
    "\n",
    "\n",
    "#r_data=np.delete(r_data,25,1)\n",
    "#r_data=np.delete(r_data,25,1)\n",
    "#r_data=np.delete(r_data,28,1)\n",
    "#r_data=np.delete(r_data,29,1)\n",
    "    return data\n",
    "\n",
    "def drop_22(a22_value,yb,data):\n",
    "    data_=data[data[:,22]==a22_value]\n",
    "    yb_=yb[data[:,22]==a22_value]\n",
    "    data_=np.delete(data_,22,1)\n",
    "    return yb_,data_\n",
    "\n",
    "def add_feature(data):\n",
    "    r_data=data.copy()#np.c_[tx,x**(i+1)]\n",
    "    r_data=np.c_[r_data,np.exp(data[:,4])]\n",
    "    r_data=np.c_[r_data,np.exp(data[:,6])]\n",
    "    r_data=np.c_[r_data,np.exp(data[:,7])]\n",
    "    #r_data=np.c_[r_data,np.exp(data[:,11])]\n",
    "    r_data=np.c_[r_data,np.exp(data[:,12])]\n",
    "    r_data=np.c_[r_data,np.exp(data[:,14])]\n",
    "    r_data=np.c_[r_data,np.exp(data[:,15])]\n",
    "    r_data=np.c_[r_data,np.exp(data[:,24])]\n",
    "    r_data=np.c_[r_data,np.exp(data[:,18])]\n",
    "    #r_data=np.c_[r_data,np.exp(data[:,11])]\n",
    "    #r_data=np.c_[r_data,np.exp(data[:,27])]\n",
    "    #r_data=np.c_[r_data,np.exp(data[:,27])]\n",
    "\n",
    "    r_data=np.c_[r_data,np.cos(data[:,10])]\n",
    "    r_data=np.c_[r_data,np.cos(data[:,24])]\n",
    "    r_data=np.c_[r_data,np.cos(data[:,27])]\n",
    "\n",
    "    r_data=np.c_[r_data,np.cos(data[:,11])]\n",
    "    r_data=np.c_[r_data,np.log(data[:,9])]\n",
    "\n",
    "    r_data=np.c_[r_data,np.log(data[:,10])]\n",
    "    r_data=np.c_[r_data,np.log(data[:,13])]\n",
    "    r_data=np.c_[r_data,np.log(data[:,19])]\n",
    "\n",
    "    #r_data=np.c_[r_data,data[:,3]*data[:,9]]\n",
    "    #r_data=np.c_[r_data,data[:,2]*data[:,19]]\n",
    "    #r_data=np.c_[r_data,data[:,22]*data[:,12]]\n",
    "    #for i in range(len(data.T)-10):\n",
    "   # for j in range(i+10,len(data.T)):\n",
    "     #       r_data=np.c_[r_data,data[:,i]*data[:,i+10]]\n",
    "    \n",
    "    r_data=np.c_[r_data,data[:,range(7,10)]**5]\n",
    "#r_data=np.c_[r_data,1/data[:,9]]\n",
    "#r_data=np.c_[r_data,1/data[:,26]]\n",
    "\n",
    "#r_data=np.c_[r_data,np.log(data[:,19])]\n",
    "#r_data=np.c_[r_data,np.log(data[:,20])]\n",
    "#r_data=np.c_[r_data,np.sin(data[:,11])]\n",
    "#r_data=np.c_[r_data,np.exp(data[:,10])]\n",
    "    return r_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]#注意.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "\n",
    "def cross_validation(feature, lable, indices, opt_model):\n",
    "    '''cross validation to evaluate performance of a given mode.\n",
    "    \n",
    "       feature (ndarray)  : feature vectors\n",
    "       lable   (array)    : lable of every row of feature\n",
    "       indices            : grouped indices of the dataset\n",
    "       opt_model(int)     : chosen classification model \n",
    "    '''\n",
    "    \n",
    "    iter_times = indices.shape[0]\n",
    "    correction = []\n",
    "    \n",
    "    for ct_iter in range(iter_times):\n",
    "        print('iteration:',ct_iter)\n",
    "        # produce training data\n",
    "        train_x = feature[indices[ct_iter]]\n",
    "        train_y = lable[indices[ct_iter]]\n",
    "        \n",
    "        # produce testing data\n",
    "        test_indices = indices[np.arange(indices.shape[0])!=ct_iter]\n",
    "        test_indices = test_indices.ravel()\n",
    "        test_x = feature[test_indices]\n",
    "        test_y = lable[test_indices]\n",
    "        \n",
    "        w, loss = train_mode(train_x, train_y, opt_model)\n",
    "        \n",
    "        prediction = predict_labels(w, test_x)\n",
    "            \n",
    "        correction.append( sum(prediction == test_y.astype(float))/test_y.shape[0] )\n",
    "        \n",
    "    return correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started the run!\n",
      "\n",
      "Finished loading data!\n",
      "\n",
      "Feature restore...\n",
      "Feature processing...\n",
      "\n",
      "Training begin...\n",
      "\n",
      "Cross validation begins...\n",
      "\n",
      "iteration: 0\n",
      "iteration: 1\n",
      "iteration: 2\n",
      "iteration: 3\n",
      "iteration: 4\n",
      "iteration: 5\n",
      "iteration: 6\n",
      "iteration: 7\n",
      "corrections\n",
      "[0.751872, 0.7513051428571429, 0.7522605714285714, 0.7518902857142857, 0.7512594285714286, 0.749408, 0.7517302857142857, 0.752352]\n",
      "Started the run!\n",
      "\n",
      "Finished loading data!\n",
      "\n",
      "Feature restore...\n",
      "Feature processing...\n",
      "\n",
      "Training begin...\n",
      "\n",
      "Cross validation begins...\n",
      "\n",
      "iteration: 0\n",
      "iteration: 1\n",
      "iteration: 2\n",
      "iteration: 3\n",
      "iteration: 4\n",
      "iteration: 5\n",
      "iteration: 6\n",
      "iteration: 7\n",
      "corrections\n",
      "[0.7516937142857143, 0.7510765714285714, 0.751488, 0.7506651428571428, 0.7521097142857143, 0.7514102857142857, 0.7512502857142858, 0.7507245714285714]\n",
      "Started the run!\n",
      "\n",
      "Finished loading data!\n",
      "\n",
      "Feature restore...\n",
      "Feature processing...\n",
      "\n",
      "Training begin...\n",
      "\n",
      "Cross validation begins...\n",
      "\n",
      "iteration: 0\n",
      "iteration: 1\n",
      "iteration: 2\n",
      "iteration: 3\n",
      "iteration: 4\n",
      "iteration: 5\n",
      "iteration: 6\n",
      "iteration: 7\n",
      "corrections\n",
      "[0.7515337142857142, 0.7510445714285714, 0.752032, 0.7512137142857143, 0.7516571428571429, 0.751456, 0.7507428571428572, 0.7519771428571429]\n",
      "Started the run!\n",
      "\n",
      "Finished loading data!\n",
      "\n",
      "Feature restore...\n",
      "Feature processing...\n",
      "\n",
      "Training begin...\n",
      "\n",
      "Cross validation begins...\n",
      "\n",
      "iteration: 0\n",
      "iteration: 1\n",
      "iteration: 2\n",
      "iteration: 3\n",
      "iteration: 4\n",
      "iteration: 5\n",
      "iteration: 6\n",
      "iteration: 7\n",
      "corrections\n",
      "[0.7505691428571428, 0.7510125714285715, 0.750592, 0.7499702857142857, 0.7520502857142857, 0.7520777142857142, 0.7536777142857143, 0.751456]\n",
      "Started the run!\n",
      "\n",
      "Finished loading data!\n",
      "\n",
      "Feature restore...\n",
      "Feature processing...\n",
      "\n",
      "Training begin...\n",
      "\n",
      "Cross validation begins...\n",
      "\n",
      "iteration: 0\n",
      "iteration: 1\n",
      "iteration: 2\n",
      "iteration: 3\n",
      "iteration: 4\n",
      "iteration: 5\n",
      "iteration: 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-fffd5c2e49d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1e-9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1e-5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1e-2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m#[1e-10,1e-8,1e-7,1e-6]:#[1e-7,1e-5,1e-2,1,100]:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcros_slice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-baf1f66d5cba>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(data_train, yb_train, opt_model, cros_vali, degree, lambda_, gamma)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cross validation begins...\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_k_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myb_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcros_vali\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0mcorrections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"corrections\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-f67033c00a02>\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(feature, lable, indices, opt_model)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mcorrection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcorrection\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#   主函数在这里\n",
    "#  opt_model=1  train_normal equation\n",
    "#  opt_model=2  train_ridge regression\n",
    "#  opt_model=3   其他的暂时有bug\n",
    "#  ...\n",
    "#  opt_model=7  Fisher Linear Discriminant\n",
    "\n",
    "opt_model=7\n",
    "cros_slice = 8\n",
    "degree=1\n",
    "lambda_=0    \n",
    "\n",
    "for gamma in [1e-9,1e-5,1e-2,1,100]:#[1e-10,1e-8,1e-7,1e-6]:#[1e-7,1e-5,1e-2,1,100]:\n",
    "    main(data_train, yb_train, opt_model, cros_slice, degree, lambda_, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
